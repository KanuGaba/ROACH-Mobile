# ROACH Mobile
### **R**oad-**O**riented **A**utonomous **C**omputerized **H**ybrid Automobile
###### By: Chappy Asel, Stephen Campbell, Kanu Gaba, Daniel Skeen
------

ROACH Mobile is ...

#### Description of Problem/Issue

⋅⋅⋅Nearly 1.3 million people die from motor vehicle accidents every year, or 3287 deaths each day. With recent introduction and widespread use of smartphones, the death toll only continues to rise. Additionally, human error and reaction time is nearly unavoidable and contributes to the large number of death from car accidents. Nonetheless, It is unsurprising that a large number of deaths comes from vehicular accidents as automobiles are the main form of day-to-day transportation. As a result, research, development, and consumer availability of autonomous vehicles have risen recently in an attempt to reduce the proportion of car accidents caused by human error. Not only this, but self-driving cars could allow for an increased amount of free time. The problem we want to tackle is to try to replicate the basic functions of autonomous vehicles with a minimal budget. <br /><br />
⋅⋅⋅Nikhil Ollukaren and Dr. Kevin McFall’s article describes in detail methods to create an affordable go-kart-like platform that would work with mini bike tires to allow for a high degree of maneuverability. This information provides us with a back-up plan in case our current vehicle, a 1987 Chrysler LeBaron, is not permitted by school officials. Additionally, this research presents a way to use simple inputs such as distance sensors and cameras paired with OpenCV computer vision to create a self-driving vehicle with minimal funding. Another article by Zelinksy describes methods using face tracking and lane monitoring to keep the car properly positioned. With nearly a third of all fatal crashes being attributed to driver inattention and fatigue, Zelinksy created a method in which he uses two stereo cameras with faceLAB software to monitor if the driver’s attention is no longer on the road, and then maintain the car within the lane lines. Finally, an article by Hikita demonstrates why ultrasonic sensors are used for backing up and parking. More research has provided us comparisons between Google and Tesla’s approach to autonomous vehicles. Google uses a 3D LIDAR sensor, which we do not plan to use, while Tesla uses 12 long-range ultrasonic sensors that provide a 360-degree field of view. After looking at the comparisons between the two different methods and components, we were able to come to the conclusion that the LIDAR sensors would not provide us with the most success due to incredibly high prices, including Google’s thirty-thousand dollar 3D LIDAR sensor. After researching different components, the extent of self-driving car accessories, and comparing different approaches, we began researching algorithms to implement lower-cost technology such as Kinect sensors. A presentation of three various methods to implement the visualization of the surroundings was conducted at the IEEE Conference on Computer Vision and Pattern Recognition in 2013. At this conference, A 2-point algorithm, a Kalman filtering method, and GPS/INS ground truth option were presented. The 2-point minimal solution motion estimation algorithm relies on two sets of point correspondences: intra-camera and inter-camera. But after discussing the results, the Kalman filtering system showed to be the most accurate. These methods would help in our calculations for the detection of moving obstacles and allowing our vehicle to avert them. <br /><br />
⋅⋅⋅Although self-driving cars may reduce the number of car accidents, they are nonetheless faulty in certain situations. Similar to the trolley problem, when the autonomous vehicle has two situations -- either to continue on its path and crash into one person, or avert the individual and crash into a group of four people -- how does the vehicle know which is the ethically correct solution? Additionally, because not all cars are self-driving, when a human-driven car approaches, the self-driving car constantly has to adjust to the human errors created by the vehicles around it. Although, most of these issues can be solved if there was a system of self-driving cars in which every car on the road was interconnected through a mesh network of sorts. This, however, is next to impossible at the current, expensive price of the self-driving vehicles. As a result, we are working to replicate these vehicles while making them affordable to the general public. <br /><br />
⋅⋅⋅While we are focusing on avoiding stationary and moving obstacles and making the vehicle app-controlled, an actual self-driving car  ready for actual roads and consumer use requires technology, artificial intelligence, and machine learning many times more capable than our own. If researched, more capabilities could be implemented onto self-driving vehicles, allowing for cars to become closer and closer to a fully autonomous state.

#### Description of Problem/Issue

⋅⋅⋅Our first goal is to develop a system of controlling a small car electronically while still allowing for human interaction. We will prioritize designing an ignition kill switch, and from there controlling braking, gas, and finally steering. Next, we will implement a wireless system (current approaches center on Bluetooth, Wi-Fi, or other radio technologies), to control the car from afar, and at the very least activate the kill switch. Once we have the vehicle in the building, we estimate we can complete this within a month or two. We have already researched this topic and have a good understanding of what is required to get this piece of the project up and running.<br /><br />
⋅⋅⋅Following the above, our goal will be to create a sensor system oriented around Kinect technology to scan the road from the front, sides, and back of the vehicle.<br />
| Level   | Description                                      | Example                                     |
| :------:|-------------------------                         | -------------------------                   |
| **0**   | Full manual control                              |                                             |
| **1**   | Function-specific                                | Assistive braking                           |
| **2**   | >=2 primary functions automated                  | Adaptive cruise control with lane centering |
|**_2.5_**| _95% automation (sans uncertain situations)_     | _Our goal_                                  |
| **3**   | 99% automation (sans safety-critical situations) | Tesla, Google etc.                          |
| **4**   | 100% automation                                  | Industy's end goal                          |
⋅⋅⋅Overall, we are aiming for Level 2.5 Automation. We want the driver to be hands-free for 90% of driving. Only in certain situations, for example, a traffic circle, would the driver resume control of the vehicle due to the complicated nature of the task at hand. One way the user would be aware of when to take control would be an implementation that feeds back an uncertainty value (between 0 and 1, l being most comfortable) to the driver, telling them when the vehicle is uncomfortable with the situation it is in.<br /><br />
⋅⋅⋅Our first objective once we have electronic control will be detecting an obstacle 80 feet away and stopping within 45 feet, proving collision avoidance at speeds up to 30 mph. This will allow us to test our setup’s reaction time compared to a human’s reaction time. We plan on completing this within 2 months of work on the project.

#### Approach

⋅⋅⋅Our general thought process when addressing the problem of autonomous driving is centered around the maxim that driving was designed for humans, so any attempt to automate the process must be emulatory of a human driver. This thought process informs our approach to each of the distinct phases of our project. The project requires two phases. The first phase is gaining remote operability, and the second phase is producing autonomous behavior. <br /><br />
⋅⋅⋅Remote operability entails gaining robotic control over the car’s general functions. For this project, the functions of the car that we will need to control are the gas, brakes, and steering. The simplest way to control brakes and gas is to emulate a human foot, so a robotic attachment connected to the pedals and floor of the car that can pull the pedal down with a motor that will not get in the way of the driver if he needs to take back control is the general approach we will take. The steering can be controlled by an additional attachment at the base of the steering wheel, where there is the most leverage. The easiest human steering motion to emulate is the push-pull-slide method. The general approach we will take is two small robotic arms attached behind the base of the wheel that have a tightening and loosening “grip” and can push and pull in an arc to make slight corrections.<br /><br />
⋅⋅⋅There are two general approaches to automating the car’s functions once remote operability has been obtained. The first approach is a rules-based approach, which is essentially a “if car sees this then do this” model, and the second approach is a machine learning approach, where a computer observes human drivers for an extensive amount of time and emulates their behavior. There is a considerable debate within the industry about which model is the more “human” approach, but we think the rules-based approach is the model that we can implement most reliably. In order for the computer to correctly adjust the car’s functions, it will have to have video data on lane lines and Doppler shift data on surrounding objects. We plan to read video data through our five Kinects and to measure Doppler shift with either ultrasonic sensors or radar or a combination of the two.<br /><br />
⋅⋅⋅The basic materials we will need are an Intel NUC computer, four Microsoft Kinects, and a single LIDAR-like sensor. In addition, we will need a vehicular platform on which to implement our design. We have already purchased a cheap car that should serve that purpose provided we receive sufficient permissions.<br /><br />
⋅⋅⋅In terms of input methods, we have a four-port USB splitter that will connect to a single the mouse, keyboard, and a single Kinect. The other 3 Kinects will interface directly with the NUC’s built-in USB 3.0 ports.<br /><br />
⋅⋅⋅Power comes from a 300W DC inverter that will run directly off of a vehicle’s own car battery. The inverter powers two 6-plug power strips. In our current preliminary implementation, the inverter is drawing around 6.5 amps at 12 volts from a power supply we are using for testing purposes.<br /><br />
⋅⋅⋅For sensing nearby objects, lane-lines, and obstacles, we are using one Kinect v2 and three Kinect v1s for side and rear observation. Most notably, the Kinect v2 has an increased camera resolution and wider viewing angle. The primary reason for using our v2 sensor our front is that its higher color camera resolution allows for better computer vision capabilities. 
Using four kinects and a LIDAR sensor, we will have the following coverage around the vehicle: The largest cone covers 45° and spans about 200 feet. This is for sensing the distance of cars at high speeds and is capable of stopping a car at speeds of up to 60 mph. The two sidewards-facing Kinect sensors have detection distances of 20 feet, allowing us to monitor blind spots and detect curbs. Additionally, we can use the color cameras to determine our distance from lane lines. When backing up, the system will need to be careful not to go at speeds of over 5 mph due to the Kinect’s limited detection distance.

#### Acknowledgments

* J4K library
* BoofCV
